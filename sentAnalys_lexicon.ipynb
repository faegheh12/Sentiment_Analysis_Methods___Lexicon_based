{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# define Sentiment Analysis methodes\n",
    "\n",
    "def AFINN(text):\n",
    "    afinn = Afinn()\n",
    "    return afinn.score(text)\n",
    "\n",
    "def SentiWordNet(pos_data):\n",
    "    sentiment = 0\n",
    "    tokens_count = 0\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            continue\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        \n",
    "        synsets = wordnet.synsets(lemma, pos=pos)\n",
    "        if not synsets:\n",
    "            continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        tokens_count += 1\n",
    "        # print(swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score())\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    if sentiment>0:\n",
    "        return \"Positive\", sentiment\n",
    "    if sentiment==0:\n",
    "        return \"Neutral\", sentiment\n",
    "    else:\n",
    "        return \"Negative\", sentiment\n",
    "\n",
    "\n",
    "def VADER(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    result = analyzer.polarity_scores(text)['compound']\n",
    "    if result >= 0.5:\n",
    "        return 'Positive', analyzer.polarity_scores(text)['pos']\n",
    "    elif result <= -0.5 :\n",
    "        return 'Negative', analyzer.polarity_scores(text)['neg']\n",
    "    else:\n",
    "        return 'Neutral', analyzer.polarity_scores(text)['neu']\n",
    "                                                         \n",
    "\n",
    "\n",
    "def Textblob(text):\n",
    "    Polarity = TextBlob(text).sentiment.polarity\n",
    "    if Polarity < 0:\n",
    "        res = 'Negative'\n",
    "    elif Polarity == 0:\n",
    "        res = 'Neutral'\n",
    "    else:\n",
    "        res = 'Positive'\n",
    "    return res, Polarity\n",
    "     \n",
    "\n",
    "                ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "def Sentiment_Analysis_lex(dataset,text_name,methods):\n",
    "\n",
    "    # dataset: path of csv file\n",
    "    # text_name: name of texts column\n",
    "    # methods: list of sentiment analysis methods\n",
    "\n",
    "    # create dataframe and Data preprocessing steps\n",
    "     \n",
    "    my_data = pd.read_csv(dataset)\n",
    "\n",
    "    # Cleaning the text\n",
    "    def clean_text(text):\n",
    "        text = re.sub('[^A-Za-z]+', ' ', text) \n",
    "        return text\n",
    "    \n",
    "    # Tokenization, POS tagging, stopwords removal\n",
    "    def Tokenization_POS_stopwords(text):\n",
    "        # POS tagger dictionary\n",
    "        pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "        tags = pos_tag(word_tokenize(text))\n",
    "        newlist = []\n",
    "        for word, tag in tags:\n",
    "            if word.lower() not in set(stopwords.words('english')):\n",
    "                newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "        return newlist\n",
    "    \n",
    "    # Lemmatization\n",
    "    def lemmatiz(pos_data):\n",
    "        lemma_rew = \" \"\n",
    "        for word, pos in pos_data:\n",
    "            if not pos: \n",
    "                lemma = word\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "            else:  \n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "        return lemma_rew\n",
    "\n",
    "    # create Required dataset\n",
    "    my_data['Cleaned_Doc'] = my_data[text_name].apply(clean_text)\n",
    "    my_data['POS_tagged'] = my_data['Cleaned_Doc'].apply(Tokenization_POS_stopwords)\n",
    "    my_data['Lemma'] = my_data['POS_tagged'].apply(lemmatiz)\n",
    "\n",
    "\n",
    "                    ####-------------------------------------------------####\n",
    "    if methods == 'all':\n",
    "        my_data['AFINN_Score'] = my_data['Cleaned_Doc'].apply(AFINN)\n",
    "        my_data[['SentiWordNet_polarity', 'SentiWordNet_Score']] = my_data['POS_tagged'].apply(lambda x: pd.Series(SentiWordNet(x)))\n",
    "        my_data[['Vader_polarity', 'vader_Score']] = my_data['Lemma'].apply(lambda x: pd.Series(VADER(x)))\n",
    "        my_data[['TextBlob_polarity', 'TextBlob_Score']] = my_data['Lemma'].apply(lambda x: pd.Series(Textblob(x)))\n",
    "    else:\n",
    "        for li in methods:\n",
    "            if li == 'AFINN':\n",
    "                my_data['AFINN_Score'] = my_data['Cleaned_Doc'].apply(AFINN)\n",
    "            if li == 'SentiWordNet':\n",
    "                my_data[['SentiWordNet_polarity', 'SentiWordNet_Score']] = my_data['POS_tagged'].apply(lambda x: pd.Series(SentiWordNet(x)))\n",
    "            if li == 'VADER':\n",
    "                my_data[['Vader_polarity', 'vader_Score']] = my_data['Lemma'].apply(lambda x: pd.Series(VADER(x)))\n",
    "            if li == 'TextBlob':\n",
    "                my_data[['TextBlob_polarity', 'TextBlob_Score']] = my_data['Lemma'].apply(lambda x: pd.Series(Textblob(x)))\n",
    "\n",
    "    final_data = my_data.drop(columns=['Cleaned_Doc','POS_tagged','Lemma'])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
